{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Data Lake Immersion\n",
    "## Lab 2.3 - Advanced Data Preparation with Developer Endpoints and Notebook\n",
    "`(Revision History:\n",
    "PA5, 2019-10-19, @akirmak: updated Section 7.2 based on feedback from identified by @rmichaud and @werberm and the solution proposed by @greenste. PA4 excluded.\n",
    "PA4, 2019-10-19, @akirmak: Advanced Spark ETL logic added as bonus\n",
    "PA3, 2019-05-09, @akirmak: updated based on feedback from @hohenber\n",
    "PA2, 2018-12-13, @akirmak \n",
    "PA1, 2018-12-07`\n",
    "\n",
    "This example shows how to do joins and filters with transforms on DynamicFrames.\n",
    "\n",
    "For purposes of our Immersion Day, we are assuming that you have done the previous Lab assignments (Create Firehose delivery stream, ingest simulated product catalogue data to S3, crawled this data and put the results into a database called `<your initials>-tame-bda-immersion-gdb` and a table called `raw` in your Data Catalog, as described in the lab guide.\n",
    "\n",
    "### 2. Getting started\n",
    "\n",
    "DataFrames APIs support elaborate methods for slicing-and-dicing the data. It includes operations such as \"selecting\" rows, columns, and cells by name or by number, filtering out rows, etc. Statistical data is usually very messy and contains lots of missing and incorrect values and range violations. So a critically important feature of DataFrames is the explicit management of missing data.\n",
    "\n",
    "We will write a script that:\n",
    "\n",
    "1. Queries data\n",
    "2. Reformats data\n",
    "3. Repartitions the data\n",
    "\n",
    "Begin by running some boilerplate to import the AWS Glue libraries we'll need and set up a single `GlueContext`.\n",
    "Then, start a Spark application and create dynamic frame from our the data in S3. \n",
    "\n",
    "Some concepts:\n",
    "\n",
    "- Spark provides a unified platform for writing big data applications, ranging from simple data loading and SQL queries to machine learning and streaming computation over the same engine and with a consistent set of APIs.\n",
    "- Spark handles loading data from Amazon S3. \n",
    "- You control your Spark Application through a driver process called the SparkSession.\n",
    "- A Spark DataFrame is the most common Structured API and simply represents a table of data with rows and columns. (Not to be confused with R and Python DataFrames. Those (with some exceptions) exist on one machine rather than multiple machines)\n",
    "- Schema is the list that defines the columns and types within those columns.\n",
    "\n",
    "**Important** Before running the next step, update the *initials* variable with your initials (e.g. fs-tame-bda-immersion-gdb for Frank Sinatra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1575272987821_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-79-94.ec2.internal:20888/proxy/application_1575272987821_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-95-147.ec2.internal:8042/node/containerlogs/container_1575272987821_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "initials = \"ADD_YOUR_INITIALS_HERE\" # <-- Add your initials here!\n",
    "\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(\n",
    "                            database = initials + \"-tame-bda-immersion-gdb\", \n",
    "                            table_name = \"raw\", \n",
    "                            transformation_ctx = \"datasource0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Schema of the Dataset\n",
    "Next, you can easily examine the schemas that the crawler recorded in the Data Catalog. For example, to see the schema of the `raw` table, run the following code:\n",
    "\n",
    "Note: To have a look at the schema, i.e. the structure of the DataFrame, we'll use the printSchema method. This will give us the different columns in our DataFrame, along with the data type and the nullable conditions for that particular column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  45000\n",
      "root\n",
      " |-- productName: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- imageUrl: string (nullable = true)\n",
      " |-- dateSoldSince: string (nullable = true)\n",
      " |-- dateSoldUntil: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+----------+------+--------------------+--------------------+--------------------+-----+-----------+----+-----+---+----+\n",
      "|         productName| product|department| color|            imageUrl|       dateSoldSince|       dateSoldUntil|price|   campaign|year|month|day|hour|\n",
      "+--------------------+--------+----------+------+--------------------+--------------------+--------------------+-----+-----------+----+-----+---+----+\n",
      "|Refined Frozen To...|    Tuna|     Music|   tan|http://lorempixel...|Tue Mar 13 2018 2...|Sat Feb 02 2019 1...|  144|BlackFriday|2018|   11| 15|  20|\n",
      "| Small Plastic Pizza|Sausages|     Games|purple|http://lorempixel...|Wed Jan 24 2018 1...|Sun Dec 16 2018 0...|   63|  10Percent|2018|   11| 15|  20|\n",
      "|Gorgeous Frozen Ball|  Towels|     Games|  blue|http://lorempixel...|Mon Feb 19 2018 1...|Sat Apr 06 2019 2...|   33|BlackFriday|2018|   11| 15|  20|\n",
      "|Unbranded Wooden ...|Sausages|     Books|orange|http://lorempixel...|Sun Jun 17 2018 0...|Tue Aug 20 2019 0...|  123|BlackFriday|2018|   11| 15|  20|\n",
      "|Handcrafted Cotto...| Chicken|  Jewelery| azure|http://lorempixel...|Wed Jul 11 2018 1...|Sun Aug 18 2019 1...|  110|  10Percent|2018|   11| 15|  20|\n",
      "+--------------------+--------+----------+------+--------------------+--------------------+--------------------+-----+-----------+----+-----+---+----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "print (\"Count: \", datasource0.count())\n",
    "\n",
    "df = datasource0.toDF()\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Selecting Multiple Columns & Filtering Data\n",
    "We can filter our data based on multiple conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+-----+-----------+\n",
      "|         productName| product|department|price|   campaign|\n",
      "+--------------------+--------+----------+-----+-----------+\n",
      "|Refined Frozen To...|    Tuna|     Music|  144|BlackFriday|\n",
      "|Gorgeous Frozen Ball|  Towels|     Games|   33|BlackFriday|\n",
      "|Unbranded Wooden ...|Sausages|     Books|  123|BlackFriday|\n",
      "|Handmade Rubber S...|    Bike|     Books|  116|BlackFriday|\n",
      "|Awesome Rubber Shoes|   Bacon|Automotive|  119|BlackFriday|\n",
      "| Generic Cotton Bike|    Tuna|     Shoes|  109|BlackFriday|\n",
      "| Refined Frozen Tuna|     Hat|    Sports|   37|BlackFriday|\n",
      "|  Sleek Frozen Pants|Computer|     Music|   58|BlackFriday|\n",
      "|Handcrafted Rubbe...|    Ball|  Outdoors|   45|BlackFriday|\n",
      "|Practical Soft Co...|  Towels|    Health|   52|BlackFriday|\n",
      "+--------------------+--------+----------+-----+-----------+"
     ]
    }
   ],
   "source": [
    "df.filter((df.campaign=='BlackFriday')).select('productName','product', 'department', 'price','campaign').limit(10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Perform transformations on data\n",
    "\n",
    "You can easily transform data.\n",
    "\n",
    "Let's only keep the fields that we want and rename `imageUrl` to `thumbnailImageUrl`. The dataset is small enough that we can look at the whole thing. The `toDF()` converts a DynamicFrame to a Spark DataFrame, so we can apply the\n",
    "transforms in SparkSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productName: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- dateSoldSince: string (nullable = true)\n",
      " |-- dateSoldUntil: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- thumbnailImageUrl: string (nullable = true)\n",
      " |-- campaignType: string (nullable = true)\n",
      "\n",
      "+--------------------+--------+----------+--------------------+--------------------+-----+----+-----+---+--------------------+------------+\n",
      "|         productName| product|department|       dateSoldSince|       dateSoldUntil|price|year|month|day|   thumbnailImageUrl|campaignType|\n",
      "+--------------------+--------+----------+--------------------+--------------------+-----+----+-----+---+--------------------+------------+\n",
      "|Refined Frozen To...|    Tuna|     Music|Tue Mar 13 2018 2...|Sat Feb 02 2019 1...|  144|2018|   11| 15|http://lorempixel...| BlackFriday|\n",
      "| Small Plastic Pizza|Sausages|     Games|Wed Jan 24 2018 1...|Sun Dec 16 2018 0...|   63|2018|   11| 15|http://lorempixel...|   10Percent|\n",
      "|Gorgeous Frozen Ball|  Towels|     Games|Mon Feb 19 2018 1...|Sat Apr 06 2019 2...|   33|2018|   11| 15|http://lorempixel...| BlackFriday|\n",
      "|Unbranded Wooden ...|Sausages|     Books|Sun Jun 17 2018 0...|Tue Aug 20 2019 0...|  123|2018|   11| 15|http://lorempixel...| BlackFriday|\n",
      "|Handcrafted Cotto...| Chicken|  Jewelery|Wed Jul 11 2018 1...|Sun Aug 18 2019 1...|  110|2018|   11| 15|http://lorempixel...|   10Percent|\n",
      "|Awesome Cotton Chips|   Chair|  Clothing|Fri Feb 23 2018 1...|Mon Jul 15 2019 0...|   50|2018|   11| 15|http://lorempixel...|        NONE|\n",
      "|Ergonomic Metal S...|   Salad|  Clothing|Mon Jun 18 2018 0...|Tue Mar 19 2019 1...|   43|2018|   11| 15|http://lorempixel...|   10Percent|\n",
      "|Handmade Rubber S...|    Bike|     Books|Fri Oct 12 2018 1...|Sun Aug 18 2019 0...|  116|2018|   11| 15|http://lorempixel...| BlackFriday|\n",
      "|Awesome Rubber Shoes|   Bacon|Automotive|Wed Jan 24 2018 2...|Wed Aug 07 2019 0...|  119|2018|   11| 15|http://lorempixel...| BlackFriday|\n",
      "|Handmade Wooden C...|    Bike| Computers|Sun Nov 11 2018 0...|Tue Jun 11 2019 2...|  149|2018|   11| 15|http://lorempixel...|        NONE|\n",
      "+--------------------+--------+----------+--------------------+--------------------+-----+----+-----+---+--------------------+------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "dsTransformed = datasource0.drop_fields(['color','hour']).rename_field('imageUrl', 'thumbnailImageUrl').rename_field('campaign', 'campaignType')\n",
    "dfTransformed = dsTransformed.toDF()\n",
    "\n",
    "dfTransformed.printSchema()\n",
    "\n",
    "dfTransformed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export transformed data to S3\n",
    "\n",
    "Let's export the transformed dataset in the previous section to S3. Convert to Parquet format. The following call writes the table across multiple files to support fast parallel reads when doing analysis later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<awsglue.dynamicframe.DynamicFrame object at 0x7f4761bb2240>"
     ]
    }
   ],
   "source": [
    "glueContext.write_dynamic_frame.from_options(frame = dsTransformed,\n",
    "              connection_type = \"s3\",\n",
    "              connection_options = {\"path\": \"s3://\" + initials + \"-tame-bda-immersion/output-etl-nb-jobs\"},\n",
    "              format = \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When execution is finished, go the the S3 folder, and verify that the files are written. For instance: the folder should look something like:\n",
    "\n",
    "`\n",
    "2018-12-07 22:42:56      87705 part-00000-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2018-12-07 22:41:55      87572 part-00000-48a202cd-86eb-4109-b3e6-f7f2bef549ef-c000.snappy.parquet\n",
    "2018-11-21 01:32:34      87572 part-00000-7f23bfb7-7a9f-4eee-bd00-4cf7ab085f57-c000.snappy.parquet\n",
    "2018-12-07 22:42:56      88180 part-00001-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2018-12-07 22:41:55      88180 part-00001-48a202cd-86eb-4109-b3e6-f7f2bef549ef-c000.snappy.parquet\n",
    "2018-11-21 01:32:34      88180 part-00001-7f23bfb7-7a9f-4eee-bd00-4cf7ab085f57-c000.snappy.parquet\n",
    "2018-12-07 22:42:56      87545 part-00002-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2018-12-07 22:41:55      87851 part-00002-48a202cd-86eb-4109-b3e6-f7f2bef549ef-c000.snappy.parquet\n",
    "2018-11-21 01:32:34      87545 part-00002-7f23bfb7-7a9f-4eee-bd00-4cf7ab085f57-c000.snappy.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Repartition Data\n",
    "**Important:** Before running the cell below, make sure you are using the correct S3 path.\n",
    "\n",
    "\n",
    "In the previous example, the data was exported to multiple S3 objects in parquet format. Since the data is small, let's combine them in a single partition.\n",
    "\n",
    "#### 7.1 Combine into a Single Partition\n",
    "To put all the history data into a single file, we need to convert it to a data frame, repartition it, and\n",
    "write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfSinglePartition = dfTransformed.repartition(1)\n",
    "dfSinglePartition.write.parquet('s3://' + initials + '-tame-bda-immersion/output-etl-nb-jobs/singlePartition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When execution is finished, go the the S3 folder, and verify that the files are written. For instance: the folder should look something like:\n",
    "\n",
    "`2018-12-07 22:55:13    1435146 part-00000-95ad4fb6-d178-47ad-8072-d60d8d8e71fd-c000.snappy.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Repartition Based on a Field\n",
    "\n",
    "Or if you want to separate it by the  `department`:\n",
    "\n",
    "**Update 2-Dec-2019:** If you get an error that the spark job is aborted, try the command with \"s3a://\" instead of \"s3://\". More details are here: https://issues.apache.org/jira/browse/HADOOP-10400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfTransformed.write.parquet(\n",
    "        's3://' + initials + '-tame-bda-immersion/output-etl-nb-jobs/byDepartment', \n",
    "        partitionBy=['department'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "Many other types of transformations could be done, such as joining tables. AWS Glue makes it easy to write it to relational databases like Redshift even with semi-structured data. It offers a transform, relationalize(), that flattens DynamicFrames no matter how complex the objects in the frame may be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting it together\n",
    "Great! We now have the final table that we'd like to use for analysis in S3, the storage layer of our Data Lake in a compact, efficient format for analytics, that we can run SQL over in AWS Glue, Athena, or Redshift Spectrum.\n",
    " \n",
    "Note that, many other types of transformations could be done (e.g. JOIN operations). We leave it to your imagination :) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Congratulations! \n",
    "You've Finished this lab. \n",
    "\n",
    "**Very Important:** SageMaker Notebooks run on EC2, and therefore you will be billed by the second unless you save your work (by downloading to your local computer) & terminate the SageMaker notebook instance. \n",
    "\n",
    "### 10. Cleaning up resources \n",
    "\n",
    "Please \n",
    " 1. download this notebook to your computer by selecting ` File -> Download as -> Notebook (.ipynb)`. \n",
    " 1. Terminate this instance. Remember that you can always recreate it from the `AWS Glue Console` by selecting the terminated instance and `Cloning` its configuration.\n",
    " \n",
    " Thank you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
